{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup and APIs in Python\n",
    "\n",
    "You know all of these data science and machine learning techniques, you're ready to take on the world. There's one problem, where do you get data? You can always access free data at sites like <a href = 'https://www.kaggle.com'>Kaggle</a>, but sometimes you have a project idea and no matter how hard you Google you can't find the exact data you need. Looks like you're out of luck.\n",
    "    \n",
    "Not so fast! There are actually a number of ways that you can find the data you're looking for and collect it in a format that you want using python. We'll learn about two techniques in this notebook, BeautifulSoup and python API wrappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "We'll start with <a href = \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">BeautifulSoup</a>. This is a wonderful package that allows you to scrape html files for the data you would like to retrieve. Before we can get going make sure that it is installed on your laptop. If you are using `pip` type `pip install beautifulsoup4`, `pip3 install beautifulsoup` if you're on a Mac. Now run the code in the next block to check that it was installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example\n",
    "\n",
    "BeautifulSoup takes in an html document and will 'parse' it for you so that you can extract the information you want. We'll start with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an html chunk\n",
    "# It has a head and a body, just like you\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_doc,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the prettify method to make our html pretty and see what it has to say\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also saved this html text as an html file in this repository. It's labeled SampleHTML.html, go ahead and open it and compare it with our `prettify()` output.\n",
    "\n",
    "As we can see from both the prettified html code and the html page, html has a lot of structure. We can exploit that structure to get the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are some examples of beautifulsoup methods that help us\n",
    "# better understand the structure of html code\n",
    "\n",
    "# What is the title of the page?\n",
    "print(soup.title)\n",
    "print()\n",
    "\n",
    "# What if I just want the text?\n",
    "print(soup.title.string)\n",
    "print()\n",
    "\n",
    "# What html structure is the title's parent?\n",
    "print(soup.title.parent.name)\n",
    "print(soup.title.parent)\n",
    "print()\n",
    "\n",
    "# What is the first a of the html document?\n",
    "print(soup.a)\n",
    "\n",
    "# What is the first a's class?\n",
    "print(soup.a['class'])\n",
    "print()\n",
    "\n",
    "# There are multiple a's can I find all of them?\n",
    "print(soup.find_all('a'))\n",
    "for a in soup.find_all('a'):\n",
    "    print()\n",
    "    print(a['class'],a.string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you practice!\n",
    "\n",
    "# Find the first p of the document\n",
    "# What is the first p's class? What string is in that p?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of the a's in the document find their href\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got some experience let's move on to code from a real webpage.\n",
    "\n",
    "### Now We're of Drinking Age\n",
    "\n",
    "I've included in this repository some html code from an <a href = \"https://untappd.com/home\">Untappd</a> search. We can read in that file with the following code. I went to untappd, and found the <a href = \"https://www.seventhsonbrewing.com\">Seventh Son</a> page then clicked on their beer list and only saved the html code from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventh_son_beer_search = open(\"SeventhSon.html\", 'r')\n",
    "\n",
    "soup = BeautifulSoup(seventh_son_beer_search, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the `prettify()` output this html code is much more complicated than our toy example from above, but `BeautifulSoup` is able to handle it all the same. Let's write some code to go through the html and grab the beer names and then store those names in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see from prettify that a div with class \"beer-container\"\n",
    "# Stores is the parent of all of every item in the code\n",
    "print(soup.div['class'])\n",
    "print()\n",
    "\n",
    "# We can also see that each beer is contained in a \"beer-item\" div\n",
    "print(soup.div.div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that single beer item. Highlight for yourself the line of html that has our beer name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <p class=\"name\"><a href=\"/b/seventh-son-brewing-company-humulus-nimbus/382779\">Humulus Nimbus </a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this is the first `p` contained in the `beer-details` `div`. So we can extract the info in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first div is for the beer container\n",
    "# The second div is for the beer item\n",
    "# The third div is for the beer details\n",
    "# The p is for the first p that contains the name\n",
    "print(soup.div.div.div.p['class'])\n",
    "# The name is stored in an a within the p\n",
    "print(soup.div.div.div.p.a.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's accomplish our initial goal of getting all the names.\n",
    "# Make an empty list\n",
    "Names = []\n",
    "\n",
    "# for each beer item in the beer container\n",
    "for div in soup.div.find_all('div'):\n",
    "    Names.append(div.p.a.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uh oh! What happened? Let's investigate\n",
    "Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well that's not good... Let's investigate more\n",
    "for div in soup.div.find_all('div'):\n",
    "    print(div.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are repeats, and not all of them are beer items?\n",
    "i = 0\n",
    "for div in soup.div.find_all('div'):\n",
    "    print(div)\n",
    "    print()\n",
    "    \n",
    "    i = i + 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `find_all` is cycling through all possible offspring of our `beer-container` `div` that themselves are `divs`. This means when a child `div` itself has children that are `divs` things get repeated. We can avoid this in two ways. One a for loop with a conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names = []\n",
    "\n",
    "for div in soup.div.find_all('div'):\n",
    "    if div['class'][0] == \"beer-item\":\n",
    "        Names.append(div.p.a.string)\n",
    "        \n",
    "Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this works as a hacky way to get what we want, it is not ideal. There is a better `BeautifulSoup` way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Names = []\n",
    "\n",
    "# Tell find_all you only want the beer-items!\n",
    "for div in soup.div.find_all('div', 'beer-item'):\n",
    "    Names.append(div.p.a.string)\n",
    "    \n",
    "Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice :-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can practice here\n",
    "# Make a list of all of the beers' styles on this html page\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some other python tools you know to describe Seventh Son \n",
    "# beers through the styles you've found\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've gotten practice with a short html string we wrote ourselves, and a very nice html page that I saved ahead of time. Now let's see how we can make soup while surfing the web.\n",
    "\n",
    "### Surfing the Web\n",
    "\n",
    "We'll scrape some data from this wikia page on the Incredible Hulk, https://marvel.fandom.com/wiki/Bruce_Banner_(Earth-616), one of my favorite super heroes. \n",
    "\n",
    "First thing we have to do is retrieve the actual html file. It's best to have python do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hulk_smash = urlopen(\"https://marvel.fandom.com/wiki/Bruce_Banner_(Earth-616)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pea_soup = BeautifulSoup(hulk_smash, \"html.parser\")\n",
    "\n",
    "# Get it? The Hulk is green, pea_soup is green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pea_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see Hulk have... not very pretty html code. This is because we're dealing with an actual webpage that includes javascript, css, and svg code. These webpages have a lot of data on them, most of which is meaningless to us. In our prior two examples we could look at the html and see for ourselves what we want to pull. Unless you have super vision that won't be possible here. So what do we do?\n",
    "\n",
    "Is it a bird or a plane, no it's your browser's developer tools! Click on the webpage link for the Hulk's wikia page. Now we'll open your browser's developer tools and see how powerful they are. Go here, https://www.wickedlysmart.com/hfhtml5/devtools.html, to get help on how to access those tools for your browser.\n",
    "\n",
    "Now let's say our goal is find out when and in which comic the Hulk first appeared. I can use my developer tools to help highlight where in the html I can find that. I'll make notes of the parents and children I'll need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "print(pea_soup.find('article','WikiaMainContent')\n",
    "      .find('div','WikiaMainContentContainer')\n",
    "      .find('div','WikiaArticle')\n",
    "      .find('div','mw-content-ltr mw-content-text')\n",
    "      .find('div','conjoined-infoboxes')\n",
    "      .find('aside',\"portable-infobox pi-background pi-theme-character pi-layout-default\")\n",
    "      .find_all('section',\"pi-item pi-group pi-border-color\")[-1]\n",
    "      .find('table')\n",
    "      .find('tbody')\n",
    "      .find('tr')\n",
    "      .find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's collect the data now\n",
    "first_app = pea_soup.find('article','WikiaMainContent').find('div','WikiaMainContentContainer').find('div','WikiaArticle').find('div','mw-content-ltr mw-content-text').find('div','conjoined-infoboxes').find('aside',\"portable-infobox pi-background pi-theme-character pi-layout-default\").find_all('section',\"pi-item pi-group pi-border-color\")[-1].find('table').find('tbody').find('tr').find_all('a')\n",
    "\n",
    "i = 0\n",
    "for a in first_app:\n",
    "    if i == 0:\n",
    "        first_app_comic = a.string\n",
    "    else:\n",
    "        first_app_date = a.string\n",
    "    i = i + 1\n",
    "    \n",
    "Hulk = {}\n",
    "Hulk['First Appearance Comic'] = first_app_comic\n",
    "Hulk['First Appearance Date'] = first_app_date\n",
    "\n",
    "print(Hulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What went wrong?\n",
    "for a in first_app:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The <i> messes up the string we can fix with .get_text()\n",
    "for a in first_app:\n",
    "    if i == 0:\n",
    "        first_app_comic = a.get_text()\n",
    "    else:\n",
    "        first_app_date = a.get_text()\n",
    "    i = i + 1\n",
    "    \n",
    "Hulk = {}\n",
    "Hulk['First Appearance Comic'] = first_app_comic\n",
    "Hulk['First Appearance Date'] = first_app_date\n",
    "\n",
    "print(Hulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this may seem silly since we can easily read this ourselves and write it down. But imagine if you worked for a data blog and you wanted to write a story about the first appearance of all of Marvel's superheroes? This allows to write a script to collect the data for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = {'Hulk':\"https://marvel.fandom.com/wiki/Bruce_Banner_(Earth-616)\",\n",
    "                'Iron Man':'https://marvel.fandom.com/wiki/Anthony_Stark_(Earth-616)',\n",
    "                'Thor':'https://marvel.fandom.com/wiki/Thor_Odinson_(Earth-616)',\n",
    "                'Captain America':'https://marvel.fandom.com/wiki/Steven_Rogers_(Earth-616)',\n",
    "                'Hawkeye':'https://marvel.fandom.com/wiki/Clinton_Barton_(Earth-616)',\n",
    "                'Black Widow':'https://marvel.fandom.com/wiki/Natalia_Romanova_(Earth-616)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_appearance(link):\n",
    "    html = urlopen(link)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    first_app = soup.find('article','WikiaMainContent').find('div','WikiaMainContentContainer').find('div','WikiaArticle').find('div','mw-content-ltr mw-content-text').find('div','conjoined-infoboxes').find('aside',\"portable-infobox pi-background pi-theme-character pi-layout-default\").find_all('section',\"pi-item pi-group pi-border-color\")[-1].find('table').find('tbody').find('tr').find_all('a')\n",
    "\n",
    "    i = 0\n",
    "    for a in first_app:\n",
    "        if i == 0:\n",
    "            first_app_comic = a.get_text()\n",
    "        else:\n",
    "            first_app_date = a.get_text()\n",
    "        i = i + 1\n",
    "        \n",
    "    return {'First Appearance Comic':first_app_comic,'First Appearance Date':first_app_date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avengers Assemble!\n",
    "Avengers = {}\n",
    "\n",
    "for character,link in characters.items():\n",
    "    Avengers[character] = get_first_appearance(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in Avengers.keys():\n",
    "    print(key,Avengers[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow `BeautifulSoup` is awesome! But as we saw in our most recent example, real websites are messy and it would be a hassle to have to go through every website using the developer tool like we just did. Luckily, people have had that very thought and developed python wrappers for APIs for various sites. We'll go over a couple of those next. But first, if you're interested in learning more about `BeautifulSoup` check out the documentation here, https://www.crummy.com/software/BeautifulSoup/bs4/doc/.\n",
    "\n",
    "## Going Ape for APIs\n",
    "\n",
    "What the heck is an API? Instead of reading through a bunch of sentences watch this video, https://www.youtube.com/watch?v=s7wmiS2mSXY, it explains the concept of an API in a great way.\n",
    "\n",
    "Now these APIs exist for many of your favorite services like Twitter, Reddit, Spotify, and more. Now for a number of sites you'd have to write your own code to interact with the API, however, for the most popular digital services someone else has already done that for us. We'll go through a couple different examples of python wrappers for APIs.\n",
    "\n",
    "### Hoop There it is\n",
    "\n",
    "Perhaps you've seen a shot chart like the one seen here, https://i.redd.it/6g9o1dwz1oc21.png. Maybe you'd like to make code to make a chart just like that, but how do you get the data? The data exists here, https://stats.nba.com, but as we said you don't want go copy and paste the data by hand, and beautiful soup code might be a hassle. Enter the package `nba_api`. This package is a python wrapper for the nba stats API. You can find its documentation here on github, https://github.com/swar/nba_api. We'll now show how we can grab nba shot chart data with `nba_api`. \n",
    "\n",
    "First make sure you install the package, if you're using `pip` run the following `pip install nba_api`, `pip3 install nba_api` if you're using a Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following to check\n",
    "import nba_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first grab the player we want\n",
    "from nba_api.stats.static import players\n",
    "\n",
    "# Returns a list of all nba players\n",
    "player_list = players.get_players()\n",
    "print(player_list[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've got a list with each entry being a dictionary\n",
    "# Let's grab data for Damian Lillard\n",
    "lillard = [player for player in player_list if player['full_name'] == 'Damian Lillard'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a player, now we can grab their shot chart data. This is done with `shotchartdetail`, I found this by browsing the documentation page, https://github.com/swar/nba_api/blob/master/docs/nba_api/stats/endpoints/shotchartdetail.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the shotchartdetail\n",
    "from nba_api.stats.endpoints import shotchartdetail\n",
    "\n",
    "lillard_shots = shotchartdetail.ShotChartDetail(player_id = lillard['id'], team_id = 0, \n",
    "                                                season_nullable = '2018-19',season_type_all_star = 'Playoffs',\n",
    "                                               last_n_games = 1,context_measure_simple = 'FGA')\n",
    "\n",
    "lillard_shots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look like the data we wanted! That's okay, what was returned was an object. Objects have methods, and luckily this particular object has a method called `get_data_frames()` that will return a pandas dataframe with the shot data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lillard_shots_df = lillard_shots.get_data_frames()[0]\n",
    "\n",
    "lillard_shots_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot!\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "shots_made = lillard_shots_df.loc[lillard_shots_df.SHOT_MADE_FLAG == 1,['LOC_X','LOC_Y']]\n",
    "shots_missed = lillard_shots_df.loc[lillard_shots_df.SHOT_MADE_FLAG == 0,['LOC_X','LOC_Y']]\n",
    "\n",
    "plt.plot(shots_made['LOC_X'], shots_made['LOC_Y'], 'b.')\n",
    "plt.plot(shots_missed['LOC_X'], shots_missed['LOC_Y'], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this doesn't look as nice as the shot chart we saw earlier, that require more code. But if you come to my plotting lecture you'll see how to make the shot charts look cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with all of Lebron's Shots\n",
    "bron = [player for player in player_list if player['full_name'] == 'LeBron James'][0]\n",
    "\n",
    "bron_shots = shotchartdetail.ShotChartDetail(player_id = bron['id'], team_id = 0,\n",
    "                                             context_measure_simple = 'FGA')\n",
    "\n",
    "bron_shots_df = bron_shots.get_data_frames()[0]\n",
    "\n",
    "# This one will look closer to a basketball court because\n",
    "# Lebron has taken a ton of shots over his career\n",
    "plt.plot(bron_shots_df['LOC_X'],bron_shots_df['LOC_Y'],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the playoff shots for Steph Curry from the 2016 finals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore nba_api more, figure out how to use another endpoint that interests you\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Read it on Reddit\n",
    "\n",
    "Now we'll step our game up even more with `praw` on to scrape Reddit. Why is this a step up? Well the nba api required no authentication. Anyone with an internet connection can scrape that data. Reddit is somewhat different. You'll need a Reddit account, and some credentials. You can set up an account here, https://www.reddit.com, and you can grab your credentials by creating an \"app\" here, https://www.reddit.com/prefs/apps. Note: In doing this you are agreeing not to use the data for commercial reasons. If you plan on developing an application in order to profit from Reddit data you need the explicit approval from Reddit. Don't mess around in this arena, you don't want a lawsuit on your hands.\n",
    "\n",
    "Also do not share your credentials with anyone! Keep these private and never share them online.\n",
    "\n",
    "I've stored my credentials on my local machine and will load them from a python file on my laptop. If you'd like to follow along with the code you'll need to enter your credentials by hand in the following block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing my credentials\n",
    "import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id, client_secret, user_agent, username, password = Credentials.GiveRedditCred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are credentials? For Reddit once you've registered an app, you'll be given a client id, and a client secret. This help Reddit know who you are and what you're authorized to do. The user agent is a string, since we're just scraping data make it say something like `android:com.example.myredditapp:v1.2.3 (by /u/kemitche)`. The username is your Reddit username.\n",
    "\n",
    "Once you've gotten credentials lets make sure you have `praw` run `pip install praw`, `pip3 install praw` if you're on a Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will admit here that I am by no means an expert in this package. I thought this might be of interest for group projects, so I set out to learn enough to introduce it here. My resources were the documentation, https://praw.readthedocs.io/en/latest/index.html, and Google.\n",
    "\n",
    "Before we can scrape data we have to make a read-only instance of Reddit. This just means we'll be accessing the Reddit API and we'll only be reading the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the read only instace\n",
    "reddit = praw.Reddit(client_id = client_id,\n",
    "                     client_secret = client_secret,\n",
    "                     user_agent = user_agent, \n",
    "                     username = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see that we correctly did it\n",
    "# You should see True\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in reddit.subreddit('aww').hot(limit=10):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can return submissions in a variety of orders\n",
    "# controversial, gilded, hot, new, rising, top\n",
    "for submission in reddit.subreddit('Columbus').controversial(limit = 5):\n",
    "    print(str(submission.author) + \": \", submission.title)\n",
    "    \n",
    "print()\n",
    "print()\n",
    "\n",
    "# I can also get all of a submission's comments\n",
    "for submission in reddit.subreddit('EarthPorn').hot(limit = 1):\n",
    "    post = submission\n",
    "    \n",
    "comments = list(post.comments)\n",
    "\n",
    "print(str(post.author) + \": \", post.title)\n",
    "i = 0\n",
    "for comment in comments:\n",
    "    if i < 10:\n",
    "        print(str(comment.author) + \": \", comment.body)\n",
    "    else:\n",
    "        break\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get more than posts!\n",
    "# We can get subreddits, redditors and more\n",
    "\n",
    "# Here's an example where we grab a subreddit\n",
    "subreddit = reddit.subreddit('math')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"title: \", subreddit.title)\n",
    "print(\"display name: \", subreddit.display_name)\n",
    "print(\"subscriber count: \", subreddit.subscribers)\n",
    "print(\"currently active users: \", subreddit.active_user_count)\n",
    "print(\"description: \", subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did I know all of that was there?\n",
    "# check out the vars() function of python\n",
    "vars(subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be all we'll learn about for `praw`. If this is interesting to you check out the documentation here, https://praw.readthedocs.io/en/latest/index.html. Learn all the things you can and can't do with `praw`!\n",
    "\n",
    "## Tweet Tweet\n",
    "\n",
    "The last python API wrapper we'll demonstrate is `tweepy`, documentation here: http://tweepy.readthedocs.org. This wrapper allows you to interact with the Twitter API. This is exciting stuff! Unfortunately, you will be unable to run the code on your personal machine because like Reddit you will need some credentials, but unlike Reddit, the process to get Twitter credentials takes longer (thanks Russia). If this is something of interest to you, you can apply for access here, https://developer.twitter.com/en/apply-for-access.html. If you explain that you're using it to learn and for academic research I've been told the process is relatively quick. Luckily I haven't had to go through this process because I gained access before this was an issue and was grandfathered in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check you have tweepy run the following\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'll grab my credentials\n",
    "consumer_key, consumer_secret, access, access_secret = Credentials.GiveTwitterCred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do all sorts of things with `tweepy` we will see a few examples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to create a Twitter Client\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab a user's timeline!\n",
    "# Here I'll print the ten most recent tweets from Tom Hanks\n",
    "for tweet in api.user_timeline('tomhanks', count = 10, tweet_mode = 'extended'):\n",
    "    print(str(tweet.user.screen_name) + \": \",tweet.full_text)\n",
    "    print(\"<3: \", tweet.favorite_count)\n",
    "    print(\"retweets: \", tweet.retweet_count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns an iterable object from twitter. You can cycle through and get a tweet object from the Twitter API. Tweet objects have a bunch of attributes. In the previous example we saw that they have a user, full_text, favorite_count, and retweet_count. You can see all of the attributes here, https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/intro-to-tweet-json.\n",
    "\n",
    "We can grab more than timelines!\n",
    "\n",
    "We can grab Profile data, friend and follower data, and query tweets. In the rest of this section we'll see how to grab a profile, a user's followers, and query tweets. We'll also learn a little about rate limits, that typically apply to all publically accessible APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we grab BillSimmons Profile\n",
    "bill = api.get_user('BillSimmons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bill.screen_name + \": \", bill.id_str)\n",
    "print(\"Followers: \", bill.followers_count)\n",
    "print(\"Following: \", bill.friends_count)\n",
    "print(\"Bio: \", bill.description)\n",
    "print(\"Location: \", bill.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can learn more about a user object in the docs.\n",
    "\n",
    "Next up we'll grab a user's followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll grab the atlantic's followers\n",
    "atlantic = api.get_user('TheAtlantic')\n",
    "atlantic_followers = atlantic.followers_count\n",
    "atlantic_id = atlantic.id\n",
    "\n",
    "print(atlantic.screen_name, \"- \", atlantic_followers, \"followers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that's a lot of followers! Now if you're thinking that it's crazy that Twitter will just hand over all of that data in an instant for free... you're right! Twitter would never do that. What you're thinking about is called firehouse access and that costs a pretty penny.\n",
    "\n",
    "The free version of the Twitter API has rate limits in place. We can only pull 15,000 followers every 15 minutes. Both versions of the API also doesn't deliver them in a nice list. They deliver them in pages with a either 100 results per page (if you want the full user) or 5000 results per page (if you're okay with just the id numbers). We'll see how to do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a paginate function that allows us\n",
    "# to cycle through the results.\n",
    "def paginate(items,n):\n",
    "    for i in range(0,len(items),n):\n",
    "        yield items[i:i+n]\n",
    "        \n",
    "# This takes in items and breaks them into chunks of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the Cursor from tweepy, this is a function that\n",
    "# imports the results from the Twitter API\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to hold the Followers\n",
    "followers = []\n",
    "\n",
    "# Put the api endpoint you'd like to use into the Cursor\n",
    "Pages = Cursor(api.followers_ids, user_id = atlantic_id).pages(2)\n",
    "\n",
    "# Go through the pages and record the followers\n",
    "for Page in Pages:\n",
    "    for follower in Page:\n",
    "        followers.append(follower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(followers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got 10000 followers, just like we planned! If we need their profile information we can grab it using tweepy, but it will take a little while because of the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in api.lookup_users(followers[:20]):\n",
    "    print(user.screen_name)\n",
    "    print(user.name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll go over one more feature of the `tweepy`, how to query tweets with keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a query\n",
    "query = \"Taco Tuesday\"\n",
    "\n",
    "# Again we use Cursor\n",
    "search = Cursor(api.search, q = query, count=100,\n",
    "                result_type=\"recent\", include_entities=True, \n",
    "                tweet_mode='extended').pages(1)\n",
    "\n",
    "# Print our results\n",
    "for Page in search:\n",
    "    for tweet in Page:\n",
    "        print(tweet.user.screen_name + \": \", tweet.full_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "We've learned a lot today. We talked about parsing html pages for data with `BeautifulSoup`. Then we talked about how some websites have handy python API wrappers that make scraping data much easier.\n",
    "\n",
    "We saw examples from saved html code, live websites, nba stats, Reddit, and Twitter. There are also python API wrappers for Facebook, Instagram, Spotify and many more. The web is full of data and as we've seen python provides away to retrieve it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
